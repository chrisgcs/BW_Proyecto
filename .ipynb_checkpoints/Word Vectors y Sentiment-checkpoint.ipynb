{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chris\\pyenviroments\\busquedaweb\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import gensim.models.wrappers.fasttext\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../fastext aligned/wiki.en.align.vec', binary=False, encoding='utf8')\n",
    "word_vectors = model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cats', 0.7321166396141052),\n",
       " ('kitten', 0.6452998518943787),\n",
       " ('dog', 0.6380450129508972),\n",
       " ('kittens', 0.6217814087867737),\n",
       " ('feline', 0.6106876730918884),\n",
       " ('cat/dog', 0.5801181793212891),\n",
       " ('pet', 0.5726068019866943),\n",
       " ('fluffykittens', 0.5583418607711792),\n",
       " ('poodle', 0.5575973391532898),\n",
       " ('felines', 0.5575810670852661)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(\"cat\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x11d169b70>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../fastext aligned/bin/wiki.en.align.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load('../fastext aligned/bin/wiki.en.align.bin', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([-0.0327,  0.0332, -0.0772,  0.0275, -0.0469,  0.1078,  0.0504,\n",
       "        -0.1214,  0.008 ,  0.0365,  0.0359, -0.0007,  0.0465, -0.0417,\n",
       "         0.0666, -0.0216,  0.0181, -0.1038,  0.0469,  0.0666,  0.0023,\n",
       "         0.1221, -0.0987,  0.0255,  0.0834,  0.0058, -0.0176,  0.063 ,\n",
       "         0.0798,  0.1121,  0.0639,  0.0554,  0.0276, -0.0525,  0.0444,\n",
       "        -0.024 , -0.0154, -0.0101,  0.0151, -0.0066, -0.0261, -0.0619,\n",
       "        -0.0529, -0.0088,  0.0302,  0.1228, -0.0194, -0.0926,  0.0387,\n",
       "        -0.0696,  0.022 , -0.0141, -0.0218, -0.0827,  0.0747,  0.0894,\n",
       "         0.0022, -0.1003,  0.0178,  0.0456,  0.0493, -0.112 ,  0.0222,\n",
       "         0.0507, -0.0145, -0.0894,  0.0295, -0.0067, -0.0302,  0.0112,\n",
       "        -0.0122, -0.0821,  0.0589, -0.0976, -0.0933,  0.0064, -0.0399,\n",
       "         0.0827, -0.0884,  0.0282, -0.0162,  0.003 ,  0.0447,  0.0874,\n",
       "         0.0849,  0.027 , -0.0394,  0.0112,  0.0571,  0.0149,  0.0287,\n",
       "        -0.046 ,  0.0819,  0.034 ,  0.0254,  0.0362, -0.066 , -0.0593,\n",
       "        -0.0612,  0.0117,  0.0418,  0.02  , -0.0587,  0.0297,  0.0097,\n",
       "        -0.0552,  0.0519,  0.0018,  0.0614, -0.0462,  0.0485, -0.0071,\n",
       "        -0.0751, -0.0124,  0.0113, -0.0366,  0.0226, -0.0462,  0.0358,\n",
       "        -0.032 , -0.0096, -0.0332,  0.0122,  0.0568,  0.0368,  0.0305,\n",
       "         0.123 ,  0.0067,  0.0736,  0.1161, -0.07  ,  0.0352, -0.0601,\n",
       "         0.0728,  0.0064, -0.0252,  0.0347, -0.0398, -0.1045,  0.0729,\n",
       "         0.0237,  0.0226,  0.047 ,  0.0257, -0.017 ,  0.0345, -0.0016,\n",
       "        -0.0457, -0.0044,  0.0682, -0.0312, -0.0625, -0.107 , -0.0309,\n",
       "         0.0345,  0.0211,  0.0607, -0.0262, -0.0034,  0.071 ,  0.0298,\n",
       "         0.0076, -0.0665,  0.0387,  0.0284, -0.049 , -0.0366, -0.0551,\n",
       "         0.0258,  0.0321, -0.0131, -0.1178, -0.0299, -0.0204,  0.0393,\n",
       "        -0.0064, -0.0198, -0.0868,  0.0114, -0.0491, -0.0421, -0.0674,\n",
       "         0.1151, -0.0756, -0.0149, -0.0024,  0.0092, -0.016 ,  0.0107,\n",
       "         0.1034, -0.002 , -0.0847, -0.01  ,  0.0823, -0.0829, -0.121 ,\n",
       "        -0.1081, -0.0757, -0.1805, -0.0399,  0.0276,  0.0227,  0.0257,\n",
       "         0.0088,  0.0037, -0.1552, -0.0029, -0.1443, -0.0511, -0.0628,\n",
       "         0.0472, -0.0561,  0.0463, -0.0303,  0.0545, -0.0201,  0.0839,\n",
       "        -0.0049, -0.0536, -0.0332, -0.0169, -0.0503, -0.1271,  0.0587,\n",
       "        -0.032 , -0.0206, -0.0345, -0.006 ,  0.0351,  0.0569,  0.0319,\n",
       "         0.0604, -0.0113, -0.0104,  0.0121,  0.0873, -0.0574,  0.0394,\n",
       "        -0.0159,  0.0671, -0.0176,  0.1006,  0.0725,  0.0796, -0.0054,\n",
       "         0.0032, -0.0566,  0.0603, -0.0502,  0.1274,  0.0282, -0.066 ,\n",
       "        -0.124 ,  0.0628,  0.0281, -0.0581,  0.0478, -0.0185,  0.0708,\n",
       "        -0.0545, -0.0044,  0.011 ,  0.0163, -0.0526, -0.0956, -0.0106,\n",
       "         0.0189,  0.0066,  0.0164, -0.0195, -0.0481, -0.0175,  0.0903,\n",
       "        -0.098 ,  0.004 , -0.142 , -0.0038, -0.061 ,  0.0866,  0.0628,\n",
       "         0.0858,  0.0176,  0.0767,  0.0346,  0.0949,  0.0192,  0.0062,\n",
       "         0.0487, -0.1055, -0.0334,  0.036 , -0.0774,  0.0243, -0.0126,\n",
       "        -0.0163, -0.0016, -0.0158,  0.0647,  0.1231, -0.0021],\n",
       "       dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_es= gensim.models.KeyedVectors.load_word2vec_format('../fastext aligned/wiki.es.align.vec', binary=False, encoding='utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_es.save('../fastext aligned/bin/wiki.es.align.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors2 = KeyedVectors.load('../fastext aligned/bin/wiki.es.align.bin', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gato', 0.5217772722244263)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors2.most_similar(positive=[word_vectors['cat']],topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar Binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load('../fastext aligned/bin/wiki.en.align.bin', mmap='r')\n",
    "word_vectors2 = KeyedVectors.load('../fastext aligned/bin/wiki.es.align.bin', mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"twits_cop25_results_en_es.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('id', 1)\n",
    "df = df.drop('created_at', 1)\n",
    "df = df.drop('geo', 1)\n",
    "df = df.drop('near', 1)\n",
    "df = df.drop('source', 1)\n",
    "df = df.drop('user_rt_id', 1)\n",
    "df = df.drop('user_rt', 1)       \n",
    "df = df.drop('retweet_id', 1)\n",
    "df = df.drop('retweet_date', 1)\n",
    "df = df.drop('translate', 1)\n",
    "df = df.drop('trans_src', 1)\n",
    "df = df.drop('trans_dest', 1)\n",
    "df = df.drop('quote_url', 1)\n",
    "df = df.drop('place', 1)\n",
    "df = df.drop('video', 1)\n",
    "df = df.drop('username', 1)\n",
    "df = df.drop('name', 1)\n",
    "df = df.drop('urls', 1)\n",
    "df = df.drop('photos', 1)\n",
    "df = df.drop('replies_count', 1)\n",
    "df = df.drop('cashtags', 1)\n",
    "df = df.drop('link', 1)\n",
    "df = df.drop('retweet', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 124524 entries, 0 to 124523\n",
      "Data columns (total 11 columns):\n",
      "conversation_id    124524 non-null int64\n",
      "date               124524 non-null object\n",
      "time               124524 non-null object\n",
      "timezone           124524 non-null int64\n",
      "user_id            124524 non-null int64\n",
      "tweet              124524 non-null object\n",
      "mentions           124524 non-null object\n",
      "retweets_count     124524 non-null int64\n",
      "likes_count        124524 non-null int64\n",
      "hashtags           124524 non-null object\n",
      "reply_to           124524 non-null object\n",
      "dtypes: int64(5), object(6)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chile = pd.read_csv(\"twits_cop25chile_results_en_es.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chile = df_chile.drop('id', 1)\n",
    "df_chile = df_chile.drop('created_at', 1)\n",
    "df_chile = df_chile.drop('geo', 1)\n",
    "df_chile = df_chile.drop('near', 1)\n",
    "df_chile = df_chile.drop('source', 1)\n",
    "df_chile = df_chile.drop('user_rt_id', 1)\n",
    "df_chile = df_chile.drop('user_rt', 1)       \n",
    "df_chile = df_chile.drop('retweet_id', 1)\n",
    "df_chile = df_chile.drop('retweet_date', 1)\n",
    "df_chile = df_chile.drop('translate', 1)\n",
    "df_chile = df_chile.drop('trans_src', 1)\n",
    "df_chile = df_chile.drop('trans_dest', 1)\n",
    "df_chile = df_chile.drop('quote_url', 1)\n",
    "df_chile = df_chile.drop('place', 1)\n",
    "df_chile = df_chile.drop('video', 1)\n",
    "df_chile = df_chile.drop('username', 1)\n",
    "df_chile = df_chile.drop('name', 1)\n",
    "df_chile = df_chile.drop('urls', 1)\n",
    "df_chile = df_chile.drop('photos', 1)\n",
    "df_chile = df_chile.drop('replies_count', 1)\n",
    "df_chile = df_chile.drop('cashtags', 1)\n",
    "df_chile = df_chile.drop('link', 1)\n",
    "df_chile = df_chile.drop('retweet', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42105 entries, 0 to 42104\n",
      "Data columns (total 11 columns):\n",
      "conversation_id    42105 non-null int64\n",
      "date               42105 non-null object\n",
      "time               42105 non-null object\n",
      "timezone           42105 non-null int64\n",
      "user_id            42105 non-null int64\n",
      "tweet              42105 non-null object\n",
      "mentions           42105 non-null object\n",
      "retweets_count     42105 non-null int64\n",
      "likes_count        42105 non-null int64\n",
      "hashtags           42105 non-null object\n",
      "reply_to           42105 non-null object\n",
      "dtypes: int64(5), object(6)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_chile.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        -3\n",
       "1        -3\n",
       "2        -3\n",
       "3        -3\n",
       "4        -3\n",
       "         ..\n",
       "124519   -3\n",
       "124520   -3\n",
       "124521   -3\n",
       "124522   -3\n",
       "124523   -3\n",
       "Name: timezone, Length: 124524, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_tweets = df.tweet\n",
    "len(lista_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Photograph', 'hellenvanmeene', 'TIME', 'emergenciaclimatica', 'horadeactuar', 'COP25', 'fridaysforfuture', 'climateprotest', 'ClimateEmergency', 'FridaysForFutureSweden', 'globalclimatestrike', 'GlobalWarming', 'GlobalGoals', 'GlobalStrike', 'globalstrikeforfuture', 'pictwittercomjdc9lTGxLL']\n",
      "Photograph by @hellenvanmeene for TIME -\n",
      "  #emergenciaclimatica #horadeactuar #COP25 #fridaysforfuture #climateprotest #ClimateEmergency #FridaysForFutureSweden #globalclimatestrike #GlobalWarming #GlobalGoals #GlobalStrike #globalstrikeforfuture pic.twitter.com/jdc9lTGxLL\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from cucco import Cucco\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words[:5]\n",
    "cucco = Cucco()\n",
    "suma = np.zeros(300)\n",
    "for i in lista_tweets:\n",
    "    #i = word_tokenize(i)\n",
    "    #l = [w for w in i if not w in stop_words]\n",
    "    l = cucco.normalize(str(i))\n",
    "    l = word_tokenize(l)\n",
    "    l = [w for w in l if not w in stop_words]\n",
    "    print(l)\n",
    "    for word in l:\n",
    "        try:\n",
    "            suma = suma + word_vectors2[word]\n",
    "        except:\n",
    "            continue\n",
    "    print(i)\n",
    "    #print(word_vectors2.similar_by_vector(suma, topn = 5 , restrict_vocab= None))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentiWordNet English Tweets World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente codigo realiza el sentiment analisis con SentiWordNet para el dataset extraido desde twitter con el hastag #Cop25 y en ingles. Para ello lee los tweets almacenados en un archivo .txt generado a partir de los csv donde estaban almacenados los tweets con su metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import statistics\n",
    "import time\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nlp = StanfordCoreNLP('stanford-corenlp-full-2018-10-05')\n",
    "\n",
    "lista_tweets = df.tweet\n",
    "tweets_clean = [] #lista de tweets de manera ordenada, sin \\n ni espacios reduntantes\n",
    "\n",
    "for tweet in lista_tweets:\n",
    "    y = ''\n",
    "    for i in tweet.strip().split('\\n'):\n",
    "        y += i\n",
    "    tweets_clean.append(y)\n",
    "\n",
    "#print(tweets_clean[:5])\n",
    "\n",
    "def analyzefile(input_tweets, output_dir, out_fileName):\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis on the text file given as input using the ANEW database.\n",
    "    Outputs results to a new CSV file in output_dir.\n",
    "    :param input_tweets: list with all the tweets\n",
    "    :param output_dir: path of directory to create new output file\n",
    "    :out_fileName: name to the output file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(output_dir, \"Output SentiWordNet \" + out_fileName + \".csv\")\n",
    "\n",
    "#     # read file into string\n",
    "#     with open(input_file, 'r', encoding= 'utf-8') as myfile:\n",
    "#         fulltext = myfile.read()\n",
    "#     # end method if file is empty\n",
    "#     if len(fulltext) < 1:\n",
    "#         print('Empty file.')\n",
    "#         return\n",
    "\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "#     sentences = tokenize.sent_tokenize(fulltext)  # split text into sentences\n",
    "    i = 1  # to store sentence index\n",
    "\n",
    "    # check each word in sentence for sentiment and write to output_file\n",
    "    with open(output_file, 'w', newline='', encoding = 'utf-8') as csvfile:\n",
    "        fieldnames = ['Sentence ID', 'Sentence', 'Sentiment', 'Sentiment Label', 'date', 'time', 'retweet_count',\n",
    "                     'likes_count', 'hashtags']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # analyze each sentence for sentiment\n",
    "        pos = 0 #posicion del tweet analizado para agregar la metadata correspondiente.\n",
    "        \n",
    "        for s in tweets_clean:\n",
    "            words = nlp.pos_tag(s.lower())\n",
    "            score_list=[]\n",
    "            for idx, t in enumerate(words):\n",
    "                newtag=''\n",
    "                lemmatized=lmtzr.lemmatize(t[0])\n",
    "                if t[1].startswith('NN'):\n",
    "                    newtag='n'\n",
    "                elif t[1].startswith('JJ'):\n",
    "                    newtag='a'\n",
    "                elif t[1].startswith('V'):\n",
    "                    newtag='v'\n",
    "                elif t[1].startswith('R'):\n",
    "                    newtag='r'\n",
    "                else:\n",
    "                    newtag='' \n",
    "                if(newtag!=''):    \n",
    "                    synsets = list(swn.senti_synsets(lemmatized, newtag))#conjunto de sinonimos\n",
    "                    #Getting average of all possible sentiments, as you requested        \n",
    "                    score=0\n",
    "                    if(len(synsets)>0):\n",
    "                        for syn in synsets:\n",
    "                            score+=syn.pos_score()-syn.neg_score()\n",
    "                        score_list.append(score/len(synsets))\n",
    "            try:\n",
    "                sentiment = statistics.mean(score_list)\n",
    "            except:\n",
    "                # write to output CSV\n",
    "                writer.writerow({'Sentence ID': i,\n",
    "                                 'Sentence': s,\n",
    "                                 'Sentiment': \"none\",\n",
    "                                 'Sentiment Label': \"none\",\n",
    "                                 'date': df.date[pos], \n",
    "                                 'time': df.time[pos], \n",
    "                                 'retweet_count': df.retweets_count[pos],\n",
    "                                 'likes_count': df.likes_count[pos], \n",
    "                                 'hashtags': df.hashtags[pos]\n",
    "                                 })\n",
    "                pos += 1\n",
    "                continue\n",
    "            label = 'neutral'\n",
    "            if sentiment > 0:\n",
    "                label = 'positive'\n",
    "            elif sentiment < 0:\n",
    "                label = 'negative'\n",
    "\n",
    "            # write to output CSV\n",
    "            writer.writerow({'Sentence ID': i,\n",
    "                             'Sentence': s,\n",
    "                             'Sentiment': sentiment,\n",
    "                             'Sentiment Label': label,\n",
    "                             'date': df.date[pos], \n",
    "                             'time': df.time[pos], \n",
    "                             'retweet_count': df.retweets_count[pos],\n",
    "                             'likes_count': df.likes_count[pos], \n",
    "                             'hashtags': df.hashtags[pos]\n",
    "                             })\n",
    "            pos += 1\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzefile(\"tweets_mundo.txt\",\"./\")\n",
    "analyzefile(tweets_clean,\"./\", \"english_test_meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124524"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Respaldo de sentiwordnet...\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import statistics\n",
    "import time\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nlp = StanfordCoreNLP('stanford-corenlp-full-2018-10-05')\n",
    "\n",
    "def analyzefile(input_file, output_dir):\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis on the text file given as input using the ANEW database.\n",
    "    Outputs results to a new CSV file in output_dir.\n",
    "    :param input_file: path of .txt file to analyze\n",
    "    :param output_dir: path of directory to create new output file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(output_dir, \"Output SentiWordNet \" + os.path.basename(input_file).rstrip('txt') + \"csv\")\n",
    "\n",
    "    # read file into string\n",
    "    with open(input_file, 'r', encoding= 'utf-8') as myfile:\n",
    "        fulltext = myfile.read()\n",
    "    # end method if file is empty\n",
    "    if len(fulltext) < 1:\n",
    "        print('Empty file.')\n",
    "        return\n",
    "\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    sentences = tokenize.sent_tokenize(fulltext)  # split text into sentences\n",
    "    i = 1  # to store sentence index\n",
    "\n",
    "    # check each word in sentence for sentiment and write to output_file\n",
    "    with open(output_file, 'w', newline='', encoding = 'utf-8') as csvfile:\n",
    "        fieldnames = ['Sentence ID', 'Sentence', 'Sentiment', 'Sentiment Label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # analyze each sentence for sentiment\n",
    "        for s in sentences:\n",
    "            words = nlp.pos_tag(s.lower())\n",
    "            score_list=[]\n",
    "            for idx, t in enumerate(words):\n",
    "                newtag=''\n",
    "                lemmatized=lmtzr.lemmatize(t[0])\n",
    "                if t[1].startswith('NN'):\n",
    "                    newtag='n'\n",
    "                elif t[1].startswith('JJ'):\n",
    "                    newtag='a'\n",
    "                elif t[1].startswith('V'):\n",
    "                    newtag='v'\n",
    "                elif t[1].startswith('R'):\n",
    "                    newtag='r'\n",
    "                else:\n",
    "                    newtag='' \n",
    "                if(newtag!=''):    \n",
    "                    synsets = list(swn.senti_synsets(lemmatized, newtag))#conjunto de sinonimos\n",
    "                    #Getting average of all possible sentiments, as you requested        \n",
    "                    score=0\n",
    "                    if(len(synsets)>0):\n",
    "                        for syn in synsets:\n",
    "                            score+=syn.pos_score()-syn.neg_score()\n",
    "                        score_list.append(score/len(synsets))\n",
    "            try:\n",
    "                sentiment = statistics.mean(score_list)\n",
    "            except:\n",
    "                # write to output CSV\n",
    "                writer.writerow({'Sentence ID': i,\n",
    "                                 'Sentence': s,\n",
    "                                 'Sentiment': \"none\",\n",
    "                                 'Sentiment Label': \"none\",\n",
    "                                 })\n",
    "                continue\n",
    "            label = 'neutral'\n",
    "            if sentiment > 0:\n",
    "                label = 'positive'\n",
    "            elif sentiment < 0:\n",
    "                label = 'negative'\n",
    "\n",
    "            # write to output CSV\n",
    "            writer.writerow({'Sentence ID': i,\n",
    "                             'Sentence': s,\n",
    "                             'Sentiment': sentiment,\n",
    "                             'Sentiment Label': label,\n",
    "                             })\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentiWordNet Spanish Tweets World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente codigo realiza el sentiment analisis con Anew para el dataset extraido desde twitter con el hastag #Cop25Chile y en espa√±ol. Para ello lee los tweets almacenados en un archivo .txt generado a partir de los csv donde estaban almacenados los tweets con su metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pi√±era y sus dichos, Patricio Navia, Schmidt y la #COP25, ahora Constanza Santa Mar√≠a y su profundo conocimiento de la realidad francesa... al parecer Chile comienza a liderar la exportaci√≥n mundial de estupidez.', 'My son, there are laws and when you break them, there are consequences.#Barroso #Montalegre #Science #water #ClimateChange #COP25 #EUCommission #UnitedNations #UNESCO #FAO #ActOnClimate #Chile #Bolivia #Argentina #Brazil #Tibet #Namibia #Zimbabwe #Congo #Portugal #Australia', 'En este #Ecocampus recordamos cap√≠tulos anteriores grabados en la #COP25 para hablar enlazar el pasado, presente y futuro de #Chile, gracias a Vicente Pizarro, a qui√©n podr√©is conocer en esta entrevista que realiza Mois√©s Palmero @GRANSEISMOüéôÔ∏èüëâüèº https://www.ivoox.com/47193128\\xa0 pic.twitter.com/t3OPa6eCwm', 'Luego de la #COP25 Chile-Madrid, en noviembre llegar√° un nuevo cap√≠tulo de negociaciones en #cambioclim√°tico en tiempos de crisis sociales y crisis del multilateralismo: #COP26 en Glasgow post-Brexit con Escocia queriendo independizarse. pic.twitter.com/LHzUvF8B1t', 'Saca la foto de la cop25 porque fue un fraude para chile, no te da un poco de verguenza']\n"
     ]
    }
   ],
   "source": [
    "tweets_clean_ch = [] #lista de tweets de manera ordenada, sin \\n ni espacios reduntantes\n",
    "\n",
    "for tweet in df_chile.tweet:\n",
    "    y = ''\n",
    "    for i in tweet.strip().split('\\n'):\n",
    "        y += i\n",
    "    tweets_clean_ch.append(y)\n",
    "\n",
    "print(tweets_clean_ch[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"spanish\"))\n",
    "anew = \"tools/Spanish_Redondo.csv\"\n",
    "\n",
    "def analyzefile_esp(input_tweets, output_dir, mode, out_fileName):\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis on the text file given as input using the ANEW database.\n",
    "    Outputs results to a new CSV file in output_dir.\n",
    "    :param input_tweets: lista con los tweets a analizar\n",
    "    :param output_dir: path of directory to create new output file\n",
    "    :param mode: determines how sentiment values for a sentence are computed (median or mean)\n",
    "    :out_fileName: nombre del archivo final\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(output_dir, \"Output Anew Sentiment \" + out_fileName + \".csv\")\n",
    "\n",
    "#     # read file into string\n",
    "#     with open(input_file, 'r', encoding = \"utf-8\") as myfile:\n",
    "#         fulltext = myfile.read()\n",
    "#     # end method if file is empty\n",
    "#     if len(fulltext) < 1:\n",
    "#         print('Empty file.')\n",
    "#         return\n",
    "\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "#     sentences = tokenize.sent_tokenize(fulltext)\n",
    "    i = 1 # to store sentence index\n",
    "    # check each word in sentence for sentiment and write to output_file\n",
    "    with open(output_file, 'w', encoding = 'utf-8') as csvfile:\n",
    "        fieldnames = ['Sentence ID', 'Sentence', 'Sentiment', 'Sentiment Label', 'Arousal', 'Dominance',\n",
    "                      '# Words Found', 'Found Words', 'All Words', 'date', 'time', 'retweet_count',\n",
    "                     'likes_count', 'hashtags']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        twt = 0 #contador de tweets para hacer calsar la meta data del df.\n",
    "        \n",
    "        # analyze each sentence for sentiment\n",
    "        for s in input_tweets:\n",
    "            # print(\"S\" + str(i) +\": \" + s)\n",
    "            all_words = []\n",
    "            found_words = []\n",
    "            total_words = 0\n",
    "            v_list = []  # holds valence scores\n",
    "            a_list = []  # holds arousal scores\n",
    "            d_list = []  # holds dominance scores\n",
    "\n",
    "            # search for each valid word's sentiment in ANEW\n",
    "            try:\n",
    "                words = nlp.pos_tag(s.lower())\n",
    "            except:\n",
    "                pos += 1\n",
    "                continue\n",
    "            for index, p in enumerate(words):\n",
    "                # don't process stops or words w/ punctuation\n",
    "                w = p[0]\n",
    "                pos = p[1]\n",
    "                if w in stops or not w.isalpha(): # check this!\n",
    "                    continue\n",
    "\n",
    "                # check for negation in 3 words before current word\n",
    "                j = index-1\n",
    "                neg = False\n",
    "                lemma = w\n",
    "                while j >= 0 and j >= index-3:\n",
    "                    if words[j][0] == 'no':\n",
    "                        neg = True\n",
    "                        break\n",
    "                    j -= 1\n",
    "\n",
    "                    lemma = stemmer.stem(w)\n",
    "\n",
    "                all_words.append(lemma.casefold())\n",
    "\n",
    "                # search for lemmatized word in ANEW\n",
    "                with open(anew) as csvfile:\n",
    "                    reader = csv.DictReader(csvfile)\n",
    "                    for row in reader:\n",
    "                        anew_lemma = stemmer.stem(row['Word'].casefold())\n",
    "                        if anew_lemma == lemma:\n",
    "                            if neg:\n",
    "                                found_words.append(\"neg-\"+lemma)\n",
    "                            else:\n",
    "                                found_words.append(lemma)\n",
    "                            v = float(row['valence'])\n",
    "                            a = float(row['arousal'])\n",
    "                            d = float(row['dominance'])\n",
    "\n",
    "                            if neg:\n",
    "                                # reverse polarity for this word\n",
    "                                v = 5 - (v - 5)\n",
    "                                a = 5 - (a - 5)\n",
    "                                d = 5 - (d - 5)\n",
    "\n",
    "                            v_list.append(v)\n",
    "                            a_list.append(a)\n",
    "                            d_list.append(d)\n",
    "\n",
    "            if len(found_words) == 0:  # no words found in ANEW for this sentence\n",
    "                writer.writerow({'Sentence ID': i,\n",
    "                                 'Sentence': s,\n",
    "                                 'Sentiment': 'N/A',\n",
    "                                 'Sentiment Label': 'N/A',\n",
    "                                 'Arousal': 'N/A',\n",
    "                                 'Dominance': 'N/A',\n",
    "                                 '# Words Found': 0,\n",
    "                                 'Found Words': 'N/A',\n",
    "                                 'All Words': all_words,\n",
    "                                 'date': df_chile.date[twt], \n",
    "                                 'time': df_chile.time[twt], \n",
    "                                 'retweet_count': df_chile.retweets_count[twt],\n",
    "                                 'likes_count': df_chile.likes_count[twt], \n",
    "                                 'hashtags': df_chile.hashtags[twt]\n",
    "                                 })\n",
    "                twt += 1\n",
    "                i += 1\n",
    "            else:  # output sentiment info for this sentence\n",
    "\n",
    "                # get values\n",
    "                if mode == 'median':\n",
    "                    sentiment = statistics.median(v_list)\n",
    "                    arousal = statistics.median(a_list)\n",
    "                    dominance = statistics.median(d_list)\n",
    "                else:\n",
    "                    sentiment = statistics.mean(v_list)\n",
    "                    arousal = statistics.mean(a_list)\n",
    "                    dominance = statistics.mean(d_list)\n",
    "\n",
    "                # set sentiment label\n",
    "                label = 'neutral'\n",
    "                if sentiment > 6:\n",
    "                    label = 'positive'\n",
    "                elif sentiment < 4:\n",
    "                    label = 'negative'\n",
    "                    \n",
    "                print(pos)\n",
    "\n",
    "                writer.writerow({'Sentence ID': i,\n",
    "                                 'Sentence': s,\n",
    "                                 'Sentiment': sentiment,\n",
    "                                 'Sentiment Label': label,\n",
    "                                 'Arousal': arousal,\n",
    "                                 'Dominance': dominance,\n",
    "                                 '# Words Found': (\"%d out of %d\" % (len(found_words), len(all_words))),\n",
    "                                 'Found Words': found_words,\n",
    "                                 'All Words': all_words,\n",
    "                                 'date': df_chile.date[twt], \n",
    "                                 'time': df_chile.time[twt], \n",
    "                                 'retweet_count': df_chile.retweets_count[twt],\n",
    "                                 'likes_count': df_chile.likes_count[twt], \n",
    "                                 'hashtags': df_chile.hashtags[twt]\n",
    "                                 })\n",
    "                twt += 1\n",
    "                i += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-2fee160bee1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# analyzefile_esp(\"tweets_chile.txt\", \"./\",\"mean\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0manalyzefile_esp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets_clean_ch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"espa√±ol_test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-120-2b94c1f1213e>\u001b[0m in \u001b[0;36manalyzefile_esp\u001b[1;34m(input_tweets, output_dir, mode, out_fileName)\u001b[0m\n\u001b[0;32m    146\u001b[0m                                  \u001b[1;34m'Found Words'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfound_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                                  \u001b[1;34m'All Words'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m                                  \u001b[1;34m'date'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdf_chile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m                                  \u001b[1;34m'time'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdf_chile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                                  \u001b[1;34m'retweet_count'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdf_chile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretweets_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jano-notebook\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jano-notebook\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4729\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4731\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4732\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '.'"
     ]
    }
   ],
   "source": [
    "# analyzefile_esp(\"tweets_chile.txt\", \"./\",\"mean\")\n",
    "analyzefile_esp(tweets_clean_ch[:5], \"./\",\"mean\", \"espa√±ol_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respaldo anew...\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"spanish\"))\n",
    "anew = \"tools/Spanish_Redondo.csv\"\n",
    "\n",
    "def analyzefile_esp(input_file, output_dir, mode):\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis on the text file given as input using the ANEW database.\n",
    "    Outputs results to a new CSV file in output_dir.\n",
    "    :param input_file: path of .txt file to analyze\n",
    "    :param output_dir: path of directory to create new output file\n",
    "    :param mode: determines how sentiment values for a sentence are computed (median or mean)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(output_dir, \"Output Anew Sentiment \" + os.path.basename(input_file).rstrip('txt') + \"csv\")\n",
    "\n",
    "    # read file into string\n",
    "    with open(input_file, 'r', encoding = \"utf-8\") as myfile:\n",
    "        fulltext = myfile.read()\n",
    "    # end method if file is empty\n",
    "    if len(fulltext) < 1:\n",
    "        print('Empty file.')\n",
    "        return\n",
    "\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    sentences = tokenize.sent_tokenize(fulltext)\n",
    "    i = 1 # to store sentence index\n",
    "    # check each word in sentence for sentiment and write to output_file\n",
    "    with open(output_file, 'w', encoding = 'utf-8') as csvfile:\n",
    "        fieldnames = ['Sentence ID', 'Sentence', 'Sentiment', 'Sentiment Label', 'Arousal', 'Dominance',\n",
    "                      '# Words Found', 'Found Words', 'All Words']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # analyze each sentence for sentiment\n",
    "        for s in sentences:\n",
    "            # print(\"S\" + str(i) +\": \" + s)\n",
    "            all_words = []\n",
    "            found_words = []\n",
    "            total_words = 0\n",
    "            v_list = []  # holds valence scores\n",
    "            a_list = []  # holds arousal scores\n",
    "            d_list = []  # holds dominance scores\n",
    "\n",
    "            # search for each valid word's sentiment in ANEW\n",
    "            try:\n",
    "                words = nlp.pos_tag(s.lower())\n",
    "            except:\n",
    "                continue\n",
    "            for index, p in enumerate(words):\n",
    "                # don't process stops or words w/ punctuation\n",
    "                w = p[0]\n",
    "                pos = p[1]\n",
    "                if w in stops or not w.isalpha(): # check this!\n",
    "                    continue\n",
    "\n",
    "                # check for negation in 3 words before current word\n",
    "                j = index-1\n",
    "                neg = False\n",
    "                lemma = w\n",
    "                while j >= 0 and j >= index-3:\n",
    "                    if words[j][0] == 'no':\n",
    "                        neg = True\n",
    "                        break\n",
    "                    j -= 1\n",
    "\n",
    "                    lemma = stemmer.stem(w)\n",
    "\n",
    "                all_words.append(lemma.casefold())\n",
    "\n",
    "                # search for lemmatized word in ANEW\n",
    "                with open(anew) as csvfile:\n",
    "                    reader = csv.DictReader(csvfile)\n",
    "                    for row in reader:\n",
    "                        anew_lemma = stemmer.stem(row['Word'].casefold())\n",
    "                        if anew_lemma == lemma:\n",
    "                            if neg:\n",
    "                                found_words.append(\"neg-\"+lemma)\n",
    "                            else:\n",
    "                                found_words.append(lemma)\n",
    "                            v = float(row['valence'])\n",
    "                            a = float(row['arousal'])\n",
    "                            d = float(row['dominance'])\n",
    "\n",
    "                            if neg:\n",
    "                                # reverse polarity for this word\n",
    "                                v = 5 - (v - 5)\n",
    "                                a = 5 - (a - 5)\n",
    "                                d = 5 - (d - 5)\n",
    "\n",
    "                            v_list.append(v)\n",
    "                            a_list.append(a)\n",
    "                            d_list.append(d)\n",
    "\n",
    "            if len(found_words) == 0:  # no words found in ANEW for this sentence\n",
    "                writer.writerow({'Sentence ID': i,\n",
    "                                 'Sentence': s,\n",
    "                                 'Sentiment': 'N/A',\n",
    "                                 'Sentiment Label': 'N/A',\n",
    "                                 'Arousal': 'N/A',\n",
    "                                 'Dominance': 'N/A',\n",
    "                                 '# Words Found': 0,\n",
    "                                 'Found Words': 'N/A',\n",
    "                                 'All Words': all_words\n",
    "                                 })\n",
    "                i += 1\n",
    "            else:  # output sentiment info for this sentence\n",
    "\n",
    "                # get values\n",
    "                if mode == 'median':\n",
    "                    sentiment = statistics.median(v_list)\n",
    "                    arousal = statistics.median(a_list)\n",
    "                    dominance = statistics.median(d_list)\n",
    "                else:\n",
    "                    sentiment = statistics.mean(v_list)\n",
    "                    arousal = statistics.mean(a_list)\n",
    "                    dominance = statistics.mean(d_list)\n",
    "\n",
    "                # set sentiment label\n",
    "                label = 'neutral'\n",
    "                if sentiment > 6:\n",
    "                    label = 'positive'\n",
    "                elif sentiment < 4:\n",
    "                    label = 'negative'\n",
    "\n",
    "                writer.writerow({'Sentence ID': i,\n",
    "                                 'Sentence': s,\n",
    "                                 'Sentiment': sentiment,\n",
    "                                 'Sentiment Label': label,\n",
    "                                 'Arousal': arousal,\n",
    "                                 'Dominance': dominance,\n",
    "                                 '# Words Found': (\"%d out of %d\" % (len(found_words), len(all_words))),\n",
    "                                 'Found Words': found_words,\n",
    "                                 'All Words': all_words\n",
    "                                 })\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
